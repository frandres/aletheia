import os
import sys
from spanish import stop_words
from sklearn.feature_extraction.text import TfidfVectorizer

def analyzer():

    def _word_ngrams(self, tokens, stop_words=None):
            """Turn tokens into a sequence of n-grams after stop words filtering"""
                    # handle stop words
                            if stop_words is not None:
                                            tokens = [w for w in tokens if w not in stop_words]

                                                    # handle token n-grams
                                                            min_n, max_n = self.ngram_range
                                                                    if max_n != 1:
                                                                                    original_tokens = tokens
                                                                                                tokens = []
                                                                                                            n_original_tokens = len(original_tokens)
                                                                                                                        for n in xrange(min_n,
                                                                                                                                                        min(max_n + 1, n_original_tokens + 1)):
                                                                                                                                                                        for i in xrange(n_original_tokens - n + 1):
                                                                                                                                                                                                tokens.append(" ".join(original_tokens[i: i + n]))

                                                                                                                                                                                                        return tokens

def main():
    folder = sys.argv[1]
    doc_names = os.listdir(folder)
    docs = [folder+'/'+x for x in doc_names]

    tfidf = TfidfVectorizer(input = 'filename',ngram_range=(1,3)
    tfidf.fit_transform(docs)

main()
