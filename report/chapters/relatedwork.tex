\section{Related works}\label{sec:related-works}

There is an increasing interest in the development of applications to generate Social Networks (SN) relating entities occurring in corpora of text. This is due to the fact that there is a growing wealth of knowledge contained in text documents which is difficult to exploit due to their unstructured nature. By generating graphs that subsume the information contained in these documents, we produce a structured view which can be analyzed using Social Network Analysis (SNA) tools. \\

In this section we present the state of the art in the generation of SN from corpora of text and some related applications that use SNA for political analysis. First in section \ref{co-occurrence} we present the entity co-occurrence approach, a simple technique that has been widely used with good results. We then show in \ref{link-characterization} relevant work by the scientific community to characterize the link between two entities in a way that can be used by SNA. We also describe in \ref{topics} methods that can be used to relate entities that despite not being mentioned in the same document might be linked by means of the broader context provided by the whole corpus of documents. Next, we present in section \ref{sna-politics} studies that use SNA for political analysis and that are related to our project. Finally, we succinctly present in \ref{framing} some considerations about how the state of the art was taken into account when making our proposal.\\

\subsection{Entity co-occurrence: a first step towards the creation of SN}\label{co-occurrence}

One of the most widely used approaches to generate SN from text consists in relating entities based on their co-occurrence in a certain context. The underlying assumption of this approach is that if two entities are consistently mentioned together then they are probably related. There are several variants depending on the granularity of the context; one can look for co-occurrence within a certain sentence, paragraph, document or cluster of documents. The choice depends on the amount of data available - finer granularity probably requires more documents to produce more relations - and on the application. \\

The authors of \cite{narcho-networks} propose a method to automatically generate a social network of narco-traffickers in Mexico based on the co-occurrence of names in books about the topic. They do Entity Recognition (ER) to produce a list of entities which is then manually curated and used to determine links between drug dealers. The weight of the relationship is the count of repetitions of the co-occurrences of two entities within a certain distance. They use different network analysis tools to show how the obtained graph closely resembles the different cartels and their chain of command. \\

Similarly, the Joint Research Centre of the European Commission has done extensive work in extracting entities and inter-entities relations from newspapers written in different countries of the European Union. In \cite{jrc-main} we find a summary of their work, which is explained in depth in \cite{jrc-2} and \cite{jrc-3}. Essentially, they look for co-occurrence of entities within previously built clusters of articles that represent a story. They take into account entity coreference and use different heuristics to improve the entity recognition and disambiguation processes. They also produced a formula to measure the strength of a link based not only on the number of co-occurrences but also the frequency of the entities in the clusters and the corpus. By doing this, they aim to weight down relationships in which one of the entities is frequently mentioned, so that only relevant relationships are chosen. \\

Another interesting aspect of their proposal is the use of Wikipedia for validating the obtained graphs. Because we are in presence of a knowledge discovery task for which we do not have a ground truth set, it is difficult to evaluate if the detected links between two entities are meaningful. The definition of a meaningful link is itself not an easy task. The authors of the Joint Research Centre look for the Wikipedia pages of the detected entities and verify if there are links between pages that correspond to the links detected by the system. They define a ``strong'' relationship as one in which there is a reciprocal presence of a link.This allows for the creation of a ground truth set which can be used to evaluate the system with the standard precision and recall metrics. \\

On a different note, the authors of  \cite{google-similarity-measure} present a technique to measure the semantic similarity of two words or phrases by using the Google search engine. They propose a metric based on information distance and Kolmogorov complexity that uses the count of search hits returned by looking up two words individually and together. They show how their approach is useful for distinguishing between colors, numbers, names of paintings and names of books, among others. \\

The main advantage of this approach is that it is able to measure the similarity of two entities based on the whole corpus of documents in the World Wide Web. The drawback is that the number of search hits returned by Google is a gross and often highly inaccurate estimate of the real count. Particularly, it is usually the case that a search with more terms returns a higher count of hits than a search with a subset of these terms. The reason for this is that when adding more terms the search is more fine-grained, allowing for a more refined estimate of documents. More specifically, when having more search terms it is necessary to go deeper through the posting lists which leads to more accurate and larger result estimates. The data centers or the indices used when answering the query also affect the number of expected hits returned. This makes approaches that depend too much on the exact count returned by the search engine unreliable. \\

There are two shortcomings in taking a co-occurrence approach. First, we are often interested in characterizing the link between two entities in terms of strength and meaning to produce richer graphs. It is true that the co-occurrence approach allows a human user to manually inspect the documents in which two entities co-occur. We are however particularly interested in mechanisms that can infer and represent the semantic nature of a link in a way exploitable by social network analysis tools with as little human participation as possible. Second, we are also interested in methods that do not rely on direct co-occurrence within a same document (or a pre-computed cluster of documents), but that can also discover meaningful relationships across a corpus.\\

\subsection{What links two entities? Enriching the SN with the strength and semantics of the relations}\label{link-characterization}

As we previously said, the co-occurrence approach relies on the assumption that if two entities are mentioned in the same context then they are probably related. Co-occurrence may indeed be suggestive that two entities are related, but if there is a relationship we need to characterize that relationship before we can perform SNA. To illustrate this, in the context of our application finding a link between a politician and a third party might indicate that they are closely aligned politically, that they are in opposition, that they participated together in a meeting, that they mentioned each other, among others. Having more information about the found relationships is consequently of great importance to be able to do better analysis.  \\

The efforts of the scientific community to characterize relationships between two entities have been mostly concentrated on Natural Language Processing methods to analyze the context in which the entities co-occur.The authors of \cite{syntactic-template} propose a method which uses dependency trees to learn patterns that relate two entities co-occurring in a sentence according to a pre-defined type of relationship. They work with two examples of relationships: `` support'' and ``meeting''. By working with a small, manually obtained number of seed instances of the relationship - tuples of entities -, they look for sentence co-occurrence in a group of news articles and extract patterns by using their SyntNet GSL algorithm over the dependencies found by a dependency parser. Similarly, in \cite{news-quotation} the authors propose a method to automatically detect quotation relationships in news articles. They aim to do this by also finding linguistic patterns that are usually used when expressing citations: quotation markers and reporting verbs. The authors of \cite{kernel-relation-extraction} illustrate the use of kernel methods for relation extraction. They first produce a shallow parse representation of the texts which is then used by kernels designed specifically to work on parse trees, which have been defined in \cite{tree-kernel}. These kernels are able to implicitly enumerate all possible subtrees of two parse trees, find which are the most common subtrees, weight them and compute a similarity measure based on these. By using a pre-obtained ground truth set, they are able to train classifiers to determine, given two entities and a tree describing the sentence they co-occur in, if the entities have relationships of the type person-affiliation and organization-location.\\

Determining the strength of a relationship is also of interest for SNA applications. To the best of our knowledge, there are no explicit efforts within the scientific community to assess the strength of relations inferred from corpora of text in a systematic way. Most of the applications are mostly concerned with link detection, which is often done by computing and theresholding similarity measures between entities. These similarity measures can be seen as measures of strength; defining a formal method to determine the strength would require however to have previously annotated SNs or a model for specifying what a strong/weak relation is. This is hard, particularly when we take into account the difficulty of manually producing measures of strength that are unbiased and inexpensive. While the question of determining the strength of relationships between two entities has been addressed for online Social Networks in which interactions may be indicative of the strength\cite{xiang2010modeling}, this remains a challenge in the area of SN generation.

\subsection{Going beyond the co-occurrence approach: finding links between entities across documents}\label{topics}

There is also a number of techniques to address the need to find links between entities without depending on their direct co-occurrence within a same document or pre-computed cluster of documents. A widely used approach is automatically inferring topics present in a corpus of text and to verify co-occurrence in articles related to these topics. The authors of  \cite{lda-topics-entities} propose a method that uses Latent Dirichlet Allocation (LDA) to produce a topic model of the documents. In LDA a document is regarded as a finite mixture of topics and represented as a vector in which each component constitutes the probability that the document belongs to a given topic. This model is then used to calculate an entity-entity measure of affinity that is used to find links between entities. The reported results are motivating; the authors show how the use of topics allows to discover more links between entities and to characterize them by means of the topics. They also report however that LDA has however one significant disadvantage: the obtained topics may be hard to interpret and may not be sufficiently semantically cohesive. \\

An alternative to the use of LDA is Latent Semantic Indexing (LSI). By producing a vectorized representation of entities (in which for instance we store information about their co-occurrence in documents), we can use Singular Value Decomposition (SVD) to find a lower dimensional space and estimate the similarity of entities based on latent concepts. This is useful for noise-reduction and for finding semantic relationships between entities. The drawback is that the found relationship may be hard to interpret. The authors of \cite{latent_semantic-index-terrorism} provide an example of the use of LSI for the generation of graphs of terrorists networks.\\

\subsection{SNA and political analysis. Has anyone done this before?}\label{sna-politics}

There are several studies that use SNA for studying the dynamics of power, particularly in legislative bodies. For example, the authors of \cite{fowler2006connecting} studied a graph of co-sponsorship of bills to study interactions between congressmen in the US House of Representatives and found that by using network analysis tools it was possible to find highly influential politicians. Similarly, in \cite{kirkland2011relational} the authors  developed a theory of influence diffusion across a legislative network of relations based on weak and strong links and found patterns useful for determining the success of a bill. In \cite{thomas2006get} we find a proposal to predict the voting of a bill based on speeches made by congressmen. \\

In \cite{chaudhari2014survey} we find a recent survey on the automatic extraction of Policy Networks. They mention several approaches for the generation of SN which we have already mentioned in this chapter (or that are at least closely related to the cited studies) and some other applications using NLP for political science analysis. For instance, in \cite{politician-location} the authors propose a method to automatically extract and characterize relationships between politicians and locations (relations, for instance, of the type (Barack Obama, President, United States); they do so by looking for co-occurrence of persons and locations in web documents, extracting keywords from their context, clustering similar pairs of (Person, Location) and identifying relevant labels. \\

In the survey great attention is paid to the work in  \cite{policy-networks}, as it is the only one, to the best of our knowledge, which specifically addresses the task of automatic Policy Network extraction. The authors of this proposal work with two PNs previously created by experts in a manual, time-consuming process. They evaluate the use of three type of metrics for SN generation that can be produced by using a Web Search Engine:

\begin{itemize}
\item \textbf{\emph{Co-occurrence metrics}}, which measure the degree to which two political actors co-occurr in web pages by looking them up individually and in conjunction in a search engine. Based on the number of results, they produce four metrics of similarity: the \emph{Jaccard Coefficient}, the \emph{Dice Coefficient}, \emph{Mutual Information} and the \emph{Google-based semantic relatedness} \cite{google-similarity-measure}.
\item \textbf{\emph{Text-based metrics}}, which use a vectorial representation of political actors in which components are the frequency of occurrence of words in a certain context of the snippets returned by a search engine. They use cosine similarity to produce a similarity measure from these vectors.
\item \textbf{\emph{Link-based metrics}}, which exploits the hyperlinks of the web pages returned by the search engines to measure the degree of association between actors. The assumption is that if two actors are mentioned in webpages that have links to the same webpages, then they are probably similar. To measure this, they use a version of the \emph{Google-based semantic relatedness}.
\end{itemize}

The manually created SNs contain positive and negative edges, which correspond to relationships of political affinity and aversion, and are annotated with measures of strength. This is particularly useful for understanding how the different methods for graph generations perform. %The authors use Pearson correlation coefficient and the Mean Squared Error to evaluate the difference between their measures of similarity and those of the manually generated SN. 
\\

In general, they obtained better results for when detecting affinity relationships than negative relationships. They also found that using link-based metrics and co-occurrence metrics are the best alternatives for positive relationships while text-based metrics are the best option for negative relationships. When comparing the four proposed measures for co-occurrence based similarity they found that \emph{Mutual Information} is the best alternative for positive links, whereas the \emph{Dice Coefficient} is the best option for negative links. \\
The main difference with our work is that while they work with a predefined list of political actors from the policy networks they use for validation, we are also interested in the discovery of relevant entities and in finding ways to characterize their links. Also, the authors of this proposal did not present any alternatives for automatically inferring the sign of the relationships detected by their system. We use parliamentary data and news articles to address these two needs.\\

\subsection{Framing our work with respect to the state of the art}\label{framing}
 
As we have seen, generating social networks from corpora of text is a topic widely addressed by the scientific community. There are two important considerations with which researchers are concerned: i) discovering the largest number of relationships possible while ensuring the discovered relationships are meaningful and ii) characterizing the relationships between entities in a way exploitable by SNA tools. \\

In the context of our application we are interested in discovering meaningful relationships between politicians and third-parties. We define a meaningful relationship between these two types of entities as a relation of political closeness, meaning that they are affected and have established positions on laws, decrees and other political decisions taken by a parliament. We present in depth our proposal in section \ref{sec:proposal}. However it is worthwhile to present in this section some considerations concerning the state of the art that were taken into account when making our proposal.\\

The task of discovering patterns of political closeness between politicians and third parties is conditioned by the fact that these relationships are usually hidden and unknown by journalists and the public. Entity co-occurrence in a given context thus entails a risk of not revealing all the relevant relationships. Similarly, characterizing the relationships by means of NLP techniques would also require knowledge by the writer of the document of the existence of a relationship between two entities. Finally, discovering topics from the corpus of documents could also lead to using topics that do not correspond to political themes. \\

To address this, we use bills as the cornerstone that allows us to link politicians and third parties. By using bills, we can model interpretable and semantically cohesive topics that allows us to address the two considerations mentioned earlier in this section. We can specifically i) detect meaningful links between entities across documents - thus increasing the precision and recall of our system in terms of an imaginary ground truth set - and ii) characterize the found links by using the bill that led to the discovery of the link.\\ 

CONTINUAR ESTO CUANDO ESTE MEJOR EXPLICADA LA PROPUESTA.
